{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNIo5y1OOmQ1ASh6wzcVVuX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Prompt Engineer Agent"],"metadata":{"id":"kk-H946frSKw"}},{"cell_type":"code","execution_count":17,"metadata":{"id":"9lPXs-lGrI93","executionInfo":{"status":"ok","timestamp":1734172341375,"user_tz":-60,"elapsed":212,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"outputs":[],"source":["import requests\n","from tqdm import tqdm\n","from typing import List, Dict, Any, Optional\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"]},{"cell_type":"markdown","source":["## 0. Setup"],"metadata":{"id":"IJyo9LrdrS_o"}},{"cell_type":"code","source":["import os\n","import yaml\n","from google.colab import drive\n","from getpass import getpass\n","from huggingface_hub import login\n","\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZpH1ZEw5rXev","executionInfo":{"status":"ok","timestamp":1734171626020,"user_tz":-60,"elapsed":16414,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"bfb2f3bc-dfa5-4038-b430-c471c4887a33"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Read YAML file\n","f_path = \"/content/drive/MyDrive/GitHub/python-codebase/machine_learning/private_keys.yml\"\n","with open(f_path, 'r') as stream:\n","    data_loaded = yaml.safe_load(stream)\n","os.environ['HF_API_TOKEN'] = data_loaded['HF_API_KEY']\n","os.environ['GITHUB_TOKEN'] = data_loaded['GITHUB_TOKEN']"],"metadata":{"id":"SAX1IqairXb_","executionInfo":{"status":"ok","timestamp":1734171626752,"user_tz":-60,"elapsed":735,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["login(token=os.environ['HF_API_TOKEN'])"],"metadata":{"id":"bFznM1iOrXYv","executionInfo":{"status":"ok","timestamp":1734171626753,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## 1. Execution"],"metadata":{"id":"9ALaeFNrrU43"}},{"cell_type":"code","source":["class LLMCaller:\n","    def __init__(self, model_name: str, use_api: bool = False, api_token: Optional[str] = None):\n","        self.model_name = model_name\n","        self.use_api = use_api\n","        self.api_token = api_token\n","\n","        if not use_api:\n","            # Load model and tokenizer locally\n","            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","        else:\n","            self.model = None  # Placeholder for API mode\n","            self.tokenizer = None\n","\n","    def query_model(self, input_text: str, system_prompt: str) -> str:\n","        \"\"\"\n","        Query the LLM with the input text and system prompt.\n","        \"\"\"\n","        combined_prompt = f\"{system_prompt}\\n{input_text}\"\n","\n","        if self.use_api:\n","            temperature = 0.1\n","            max_tokens = 50\n","            top_p = 0.1\n","\n","            headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\n","            response = requests.post(\n","                f\"https://api-inference.huggingface.co/models/{self.model_name}\",\n","                headers=headers,\n","                json={\n","                    \"inputs\": combined_prompt,\n","                    \"parameters\": {\n","                        \"temperature\": temperature,\n","                        \"max_tokens\": max_tokens,\n","                        \"top_p\": top_p\n","                      }\n","                    }\n","            )\n","            response.raise_for_status()\n","            result = response.json()\n","\n","            # The response is typically a list of generated texts\n","            if isinstance(result, list) and len(result) > 0:\n","                return result[0][\"generated_text\"].replace(combined_prompt, \"\", 1).strip()\n","            else:\n","                raise ValueError(\"Unexpected response format from Hugging Face API.\")\n","        else:\n","            inputs = self.tokenizer(combined_prompt, return_tensors=\"pt\")\n","            #outputs = self.model.generate(**inputs, max_new_tokens=128, num_return_sequences=1, do_sample=True)\n","            #return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            outputs = self.model.generate(**inputs, max_length=inputs[\"input_ids\"].shape[1] + 50)\n","            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            return full_output.replace(combined_prompt, \"\", 1).strip()\n","\n","class PromptEngineerLLM:\n","    def __init__(self, model_name: str, use_api: bool = False, api_token: Optional[str] = None):\n","        self.model_name = model_name\n","        self.use_api = use_api\n","        self.api_token = api_token\n","        self.memory = []  # Memory to store previous input-output pairs and prompts\n","        self.llm_caller = LLMCaller(model_name, use_api, api_token)\n","\n","    def generate_prompt(self, task_description: str, system_prompt: str, outputs: List[Dict[str, Any]]) -> str:\n","        \"\"\"\n","        Generate a system prompt based on task description, current system prompt, and outputs.\n","        \"\"\"\n","        memory_context = \"\\n\".join([\n","            f\"Input: {entry['input']}, Output: {entry['output']}, Target: {entry['target']}\"\n","            for entry in self.memory\n","        ])\n","\n","        prompt = (\n","            f\"You are an expert prompt engineer.\\n\"\n","            f\"Based on the following memory:\\n{memory_context}\\n\"\n","            f\"and the current task description:\\n{task_description}\\n\"\n","            f\"and the current system prompt:\\n{system_prompt}\\n\"\n","            f\"Optimize the system prompt to achieve better alignment with the target outputs.\\n\"\n","            f\"Only provide the new system_prompt, nothing else\\n\"\n","            f\"Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without System Prompt: or system_prompt:)\"\n","        )\n","        return prompt\n","\n","    def evaluate_system_prompt(self, system_prompt: str, batch: List[Dict[str, Any]]) -> float:\n","        \"\"\"\n","        Evaluate the quality of the system prompt based on batch performance.\n","        \"\"\"\n","        correct_count = 0\n","        for entry in tqdm(batch):\n","            input_text = entry['input']\n","            target_output = entry['target']\n","            model_output = self.llm_caller.query_model(input_text, system_prompt)\n","\n","            if model_output.strip() == target_output.strip():\n","                correct_count += 1\n","\n","        return correct_count / len(batch)\n","\n","    def optimize_prompt(self, task_description: str, system_prompt: str, batch: List[Dict[str, Any]]) -> str:\n","        \"\"\"\n","        Optimize the system prompt using the task description, existing system prompt, and batch.\n","        \"\"\"\n","        # Update memory with the current batch\n","        self.memory.extend(batch)\n","\n","        # Generate input\n","        new_input = self.generate_prompt(task_description, system_prompt, batch)\n","        print()\n","        print(\"*\"*50)\n","        print(f\"New input:\\n\\n {new_input}\")\n","        print(\"*\"*50)\n","        print()\n","\n","        # Generate new system prompt\n","        new_system_prompt = self.llm_caller.query_model(new_input, \"\")\n","        if True:\n","          new_system_prompt = (\n","              new_system_prompt\n","              .replace(\"system_prompt:\", \"\")\n","              .replace(\"System Prompt:\", \"\")\n","              .replace(\"Optimized Prompt:\", \"\")\n","              .replace(\"New system prompt:\", \"\")\n","          )\n","        print()\n","        print(\"*\"*50)\n","        print(f\"New system prompt:\\n\\n {new_system_prompt}\")\n","        print(\"*\"*50)\n","        print()\n","\n","        # Evaluate the new system prompt\n","        performance = self.evaluate_system_prompt(new_system_prompt, batch)\n","        print()\n","        print(\"*\"*50)\n","        print(f\"New system prompt performance:\\n\\n {performance}\")\n","        print(\"*\"*50)\n","        print()\n","\n","        return new_system_prompt"],"metadata":{"id":"1jY97wdTrNiA","executionInfo":{"status":"ok","timestamp":1734178575693,"user_tz":-60,"elapsed":211,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":["### 1.1. Example loading models in memory"],"metadata":{"id":"TkCbf4IXs03o"}},{"cell_type":"code","source":["# Parameters\n","model_name = \"gpt2\"\n","#model_name = \"distilgpt2\"\n","task_description = \"Task: Summarize the text.\" # Fixed\n","system_prompt = \"Summarize the following sentences.\" # Prompt engineering\n","\n","# Dataset\n","batch = [\n","  {\"input\": \"The cat sat on the mat.\", \"output\": None, \"target\": \"The cat sat.\"},\n","  {\"input\": \"The quick brown fox jumps over the lazy dog.\", \"output\": None, \"target\": \"The fox jumps.\"},\n","  {\"input\": \"A journey of a thousand miles begins with a single step.\", \"output\": None, \"target\": \"A journey begins with a step.\"},\n","  {\"input\": \"To be or not to be, that is the question.\", \"output\": None, \"target\": \"To be or not to be.\"},\n","  {\"input\": \"All that glitters is not gold.\", \"output\": None, \"target\": \"Not all that glitters is gold.\"}\n","]"],"metadata":{"id":"ip0rtp2Ss3bu","executionInfo":{"status":"ok","timestamp":1734178659851,"user_tz":-60,"elapsed":214,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["# Initialize the LLM summarizer\n","llm_caller = LLMCaller(model_name, use_api=False)\n","\n","# Initialize the Prompt Engineer LLM\n","prompt_engineer = PromptEngineerLLM(model_name, use_api=False)"],"metadata":{"id":"yAKxRMmiw_4S","executionInfo":{"status":"ok","timestamp":1734178664949,"user_tz":-60,"elapsed":2667,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":107,"outputs":[]},{"cell_type":"code","source":["# Get summarizer outputs & update dataset\n","batch_new = []\n","for example in tqdm(batch):\n","  example[\"output\"] = llm_caller.query_model(example[\"input\"], system_prompt)\n","  batch_new += [example]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJ6BP7NNtPMX","executionInfo":{"status":"ok","timestamp":1734178682163,"user_tz":-60,"elapsed":16008,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"82965fd4-b11d-4963-f700-c5fc0e0ee010"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 20%|██        | 1/5 [00:02<00:11,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 40%|████      | 2/5 [00:05<00:08,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 60%|██████    | 3/5 [00:08<00:05,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 80%|████████  | 4/5 [00:12<00:03,  3.12s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","100%|██████████| 5/5 [00:15<00:00,  3.15s/it]\n"]}]},{"cell_type":"code","source":["example"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arvlRNj4uN-u","executionInfo":{"status":"ok","timestamp":1734178682163,"user_tz":-60,"elapsed":4,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"862f1808-ba83-4de1-c6b9-652b7f50417d"},"execution_count":109,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input': 'All that glitters is not gold.',\n"," 'output': 'The gold is not gold.\\nThe gold is not gold.\\nThe gold is not gold.\\nThe gold is not gold.\\nThe gold is not gold.\\nThe gold is not gold.\\nThe gold is not gold.',\n"," 'target': 'Not all that glitters is gold.'}"]},"metadata":{},"execution_count":109}]},{"cell_type":"code","source":["# Optimize the system prompt\n","optimized_prompt = prompt_engineer.optimize_prompt(task_description, system_prompt, batch_new)\n","print(f\"Optimized Prompt:\\n{optimized_prompt}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQdo2QSvuOBN","executionInfo":{"status":"ok","timestamp":1734178710041,"user_tz":-60,"elapsed":27881,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"87e9dbd9-9e7e-4e32-9c40-d0b078549b02"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","**************************************************\n","New input:\n","\n"," You are an expert prompt engineer.\n","Based on the following memory:\n","Input: The cat sat on the mat., Output: The cat sat on the mat.\n","The cat sat on the mat.\n","The cat sat on the mat.\n","The cat sat on the mat.\n","The cat sat on the mat.\n","The cat sat on the mat.\n","The, Target: The cat sat.\n","Input: The quick brown fox jumps over the lazy dog., Output: The lazy dog jumps over the lazy dog.\n","The lazy dog jumps over the lazy dog.\n","The lazy dog jumps over the lazy dog.\n","The lazy dog jumps over the lazy dog.\n","The lazy dog jumps over the lazy dog., Target: The fox jumps.\n","Input: A journey of a thousand miles begins with a single step., Output: A journey of a thousand miles begins with a single step.\n","A journey of a thousand miles begins with a single step.\n","A journey of a thousand miles begins with a single step.\n","A journey of a thousand miles begins with a single, Target: A journey begins with a step.\n","Input: To be or not to be, that is the question., Output: The question is, what is the question?\n","The question is, what is the question?\n","The question is, what is the question?\n","The question is, what is the question?\n","The question is, what is the question?, Target: To be or not to be.\n","Input: All that glitters is not gold., Output: The gold is not gold.\n","The gold is not gold.\n","The gold is not gold.\n","The gold is not gold.\n","The gold is not gold.\n","The gold is not gold.\n","The gold is not gold., Target: Not all that glitters is gold.\n","and the current task description:\n","Task: Summarize the text.\n","and the current system prompt:\n","Summarize the following sentences.\n","Optimize the system prompt to achieve better alignment with the target outputs.\n","Only provide the new system_prompt, nothing else\n","Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without System Prompt: or system_prompt:)\n","**************************************************\n","\n","\n","**************************************************\n","New system prompt:\n","\n"," Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without  or )\n","Only provide one system_prompt, and only provide the\n","**************************************************\n","\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 0/5 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 20%|██        | 1/5 [00:05<00:21,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 40%|████      | 2/5 [00:11<00:16,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 60%|██████    | 3/5 [00:14<00:09,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"," 80%|████████  | 4/5 [00:17<00:04,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","100%|██████████| 5/5 [00:21<00:00,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","**************************************************\n","New system prompt performance:\n","\n"," 0.0\n","**************************************************\n","\n","Optimized Prompt:\n","Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without  or )\n","Only provide one system_prompt, and only provide the\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["out = llm_caller.query_model(example[\"input\"], optimized_prompt)\n","print(optimized_prompt)\n","print(example[\"input\"])\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5pZiDcNGsNy","executionInfo":{"status":"ok","timestamp":1734178715706,"user_tz":-60,"elapsed":3624,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"1b015703-111a-470a-8ef0-7b63c14235ac"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without  or )\n","Only provide one system_prompt, and only provide the\n","All that glitters is not gold.\n","The only thing that glitters is not gold.\n","The only thing that glitters is not gold.\n","The only thing that glitters is not gold.\n","The only thing that glitters is not gold.\n","The only thing that gl\n"]}]},{"cell_type":"markdown","source":["### 1.2. Using the request API"],"metadata":{"id":"8rSboNsxxiKL"}},{"cell_type":"code","source":["# Parameters\n","#model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","model_name = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n","#model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n","task_description = \"Task: Summarize the text.\" # Fixed\n","system_prompt = \"Summarize the following sentences.\" # Prompt engineering\n","\n","# Dataset\n","batch = [\n","  {\"input\": \"The cat sat on the mat.\", \"output\": None, \"target\": \"The cat sat.\"},\n","  {\"input\": \"The quick brown fox jumps over the lazy dog.\", \"output\": None, \"target\": \"The fox jumps.\"},\n","  {\"input\": \"A journey of a thousand miles begins with a single step.\", \"output\": None, \"target\": \"A journey begins with a step.\"},\n","  {\"input\": \"To be or not to be, that is the question.\", \"output\": None, \"target\": \"To be or not to be.\"},\n","  {\"input\": \"All that glitters is not gold.\", \"output\": None, \"target\": \"Not all that glitters is gold.\"}\n","]"],"metadata":{"id":"yVPD4SWpuODt","executionInfo":{"status":"ok","timestamp":1734173250080,"user_tz":-60,"elapsed":224,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Initialize the LLM summarizer\n","llm_caller = LLMCaller(model_name, use_api=True, api_token=os.environ['HF_API_TOKEN'])\n","\n","# Initialize the Prompt Engineer LLM\n","prompt_engineer = PromptEngineerLLM(model_name, use_api=True, api_token=os.environ['HF_API_TOKEN'])"],"metadata":{"id":"DKwfF2R9uOF-","executionInfo":{"status":"ok","timestamp":1734178581513,"user_tz":-60,"elapsed":246,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}}},"execution_count":101,"outputs":[]},{"cell_type":"code","source":["# Prueba\n","input_text = \"All that glitters is not gold.\"\n","system_example = \"Summarize the following sentences.\"\n","llm_caller.query_model(input_text, system_example)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"t2dYqagK9b7-","executionInfo":{"status":"ok","timestamp":1734176693060,"user_tz":-60,"elapsed":1630,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"80201680-e183-48a0-f5fe-b97d1a24fa90"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The world is full of people who are not what they seem.\\n\\nNot everything that looks valuable is actually valuable.\\nThe world is full of people who are not as they appear.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# Get summarizer outputs & update dataset\n","batch_new = []\n","for example in tqdm(batch):\n","  example[\"output\"] = llm_caller.query_model(example[\"input\"], system_prompt)\n","  batch_new += [example]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zqg96vmeuOIG","executionInfo":{"status":"ok","timestamp":1734176737025,"user_tz":-60,"elapsed":22087,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"ec3d94b1-d8c7-498c-d815-92a3a079849e"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:21<00:00,  4.36s/it]\n"]}]},{"cell_type":"code","source":["input_text = example[\"input\"]\n","combined_prompt = f\"{system_prompt}\\n{input_text}\"\n","print(combined_prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwlLabck8ZBF","executionInfo":{"status":"ok","timestamp":1734176737025,"user_tz":-60,"elapsed":11,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"f36bb25b-ac5e-4f70-feb1-162d129781cb"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Summarize the following sentences.\n","All that glitters is not gold.\n"]}]},{"cell_type":"code","source":["system_prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Yl9WduQg8w4m","executionInfo":{"status":"ok","timestamp":1734176737026,"user_tz":-60,"elapsed":11,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"8e55b0b1-96cd-49ed-85b9-293ad0b0aa47"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Summarize the following sentences.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["example"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYNK-hNyyItz","executionInfo":{"status":"ok","timestamp":1734176737026,"user_tz":-60,"elapsed":9,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"840226a0-6646-4268-e8b4-ccf7ca13dd16"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input': 'All that glitters is not gold.',\n"," 'output': 'The world is full of people who are not what they seem.\\n\\nNot everything that looks valuable is actually valuable.\\nThe world is full of people who are not as they appear.',\n"," 'target': 'Not all that glitters is gold.'}"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["# Optimize the system prompt\n","optimized_prompt = prompt_engineer.optimize_prompt(task_description, system_prompt, batch_new)\n","print(f\"Optimized Prompt:\\n{optimized_prompt}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WG31cnL_yIwQ","executionInfo":{"status":"ok","timestamp":1734178586250,"user_tz":-60,"elapsed":1483,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"f395dde6-d77f-47be-b4ba-0fc74ed0abc9"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","**************************************************\n","New input:\n","\n"," You are an expert prompt engineer.\n","Based on the following memory:\n","Input: The cat sat on the mat., Output: The dog chased the cat. The cat jumped off the mat. The dog ran after the cat. The cat climbed up the tree. The dog barked at the cat. The cat meowed at the dog. The dog whined. The cat purred. The dog whimpered.\n","\n","The cat sat on the mat, but when the dog chased it, the cat jumped off the mat and ran up a tree. The dog barked at the cat, which meowed back. The dog whined and then whimpered., Target: The cat sat.\n","Input: The quick brown fox jumps over the lazy dog., Output: The fox is quick and brown.\n","The dog is lazy.\n","\n","The quick brown fox jumps over the lazy dog, with the fox being quick and brown and the dog being lazy., Target: The fox jumps.\n","Input: A journey of a thousand miles begins with a single step., Output: The journey of a thousand miles begins with a single step.\n","\n","The journey of a thousand miles begins with a single step., Target: A journey begins with a step.\n","Input: To be or not to be, that is the question., Output: Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and, by opposing, end them.\n","To die, to sleep, no more, and by a sleep to say we end the heartache and the thousand natural shocks that flesh is heir to.\n","'Tis a consummation devoutly to be wished.\n","To die, to sleep, to sleep, perchance to dream, ay there's the rub, for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause.\n","There's the respect that makes calamity of so long life.\n","For who would bear the whips and scorns of time, the oppressor's wrong, the proud man's contumely, the pangs of despised love, the law's delay, the insolence of office, and the spurns that patient merit of the unworthy takes, when he himself might his quietus make with a bare bodkin?\n","Who would fardels bear, to grunt and sweat under a weary life, but that the dread of something after death, the undiscovered country from whose bourn no traveller returns, puzzles the will and makes us rather bear those ills we have than fly to others that we know not of?\n","Thus conscience does make cowards of us all, and thus the native hue of resolution is sicklied o'er with the pale cast of thought, and enterprises of great pitch and moment with this regard their currents turn awry and lose the name of action.\n","Soft you now, the fair Ophelia, nymph, in thy orisons be all my sins remembered.\n","\n","The speaker contemplates the question of whether it is better to be alive or dead. He considers the possibility of dying to escape the troubles of life, but also wonders about the dreams that might come in death. He also considers the fear of the unknown that keeps people from choosing death. Ultimately, he concludes that fear and thought can prevent people from taking action. He then addresses Ophelia, asking her to remember him in her prayers., Target: To be or not to be.\n","Input: All that glitters is not gold., Output: The world is full of people who are not what they seem.\n","\n","Not everything that looks valuable is actually valuable.\n","The world is full of people who are not as they appear., Target: Not all that glitters is gold.\n","and the current task description:\n","Task: Summarize the text.\n","and the current system prompt:\n","Summarize the following sentences.\n","Optimize the system prompt to achieve better alignment with the target outputs.\n","Only provide the new system_prompt, nothing else\n","Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without System Prompt: or system_prompt:)\n","**************************************************\n","\n","\n","**************************************************\n","New system prompt:\n","\n"," \n","\"Identify the main idea of the following text, focusing on the most important and valuable information. Avoid unnecessary details and summarize in a concise and clear manner.\"\n","\n","\n","\"Summarize the following text, highlighting the most important and valuable information while avoiding unnecessary details. Ensure the summary is concise and clear.\"\n","**************************************************\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:01<00:00,  4.80it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","**************************************************\n","New system prompt performance:\n","\n"," 0.0\n","**************************************************\n","\n","Optimized Prompt:\n","\n","\"Identify the main idea of the following text, focusing on the most important and valuable information. Avoid unnecessary details and summarize in a concise and clear manner.\"\n","\n","\n","\"Summarize the following text, highlighting the most important and valuable information while avoiding unnecessary details. Ensure the summary is concise and clear.\"\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["out = llm_caller.query_model(example[\"input\"], optimized_prompt)\n","print(optimized_prompt)\n","print(example[\"input\"])\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzq9w8YMF1ix","executionInfo":{"status":"ok","timestamp":1734178632833,"user_tz":-60,"elapsed":488,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"af79c4cc-73f4-4b11-81ee-88ed932096b7"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\"Identify the main idea of the following text, focusing on the most important and valuable information. Avoid unnecessary details and summarize in a concise and clear manner.\"\n","\n","\n","\"Summarize the following text, highlighting the most important and valuable information while avoiding unnecessary details. Ensure the summary is concise and clear.\"\n","All that glitters is not gold.\n","This well-known saying has been passed down through generations and remains relevant today. It serves as a reminder that not everything that appears valuable or attractive is truly so. The saying can be applied to various aspects of life, including material possessions, relationships, and personal goals.\n","\n","When it comes to material possessions, the saying encourages us to look beyond the surface and consider the true worth of an item. For example, a diamond ring may appear valuable and attractive, but if it is a fake, then it holds little real value. Similarly, a car may look sleek and impressive, but if it is poorly made and constantly breaks down, then it is not truly valuable.\n","\n","The saying can also be applied to relationships. Just because someone is charming and attractive does not mean they are a good person to be around. They may have ulterior motives or be harmful to our well-being. It is important to look beyond the surface and consider the true nature of the relationship.\n","\n","Finally, the saying can be applied to personal goals. We may have dreams and aspirations that appear valuable and attractive, but if they are not aligned with our true values and passions, then they are not truly worth pursuing. It is important to consider what is truly important to us and pursue goals that align with our authentic selves.\n","\n","In conclusion, the saying \"all that glitters is not gold\" serves as a valuable reminder to look beyond the surface and consider the true worth of things, whether it be material possessions, relationships, or personal goals. By doing so, we can make more informed decisions and pursue what is truly valuable and meaningful to us.\n"]}]},{"cell_type":"code","source":["# Example\n","input_text = f\"\"\"\n"," You are an expert prompt engineer.\n","Based on the following memory:\n","Input: The cat sat on the mat., Output: The dog chased the cat. The cat jumped off the mat. The dog ran after the cat. The cat climbed up the tree. The dog barked at the cat. The cat meowed at the dog. The dog whined. The cat purred. The dog whimpered.\n","\n","The cat sat on the mat, but when the dog chased it, the cat jumped off the mat and ran up a tree. The dog barked at the cat, which meowed back. The dog whined and then whimpered., Target: The cat sat.\n","Input: The quick brown fox jumps over the lazy dog., Output: The fox is quick and brown.\n","The dog is lazy.\n","\n","The quick brown fox jumps over the lazy dog, with the fox being quick and brown and the dog being lazy., Target: The fox jumps.\n","Input: A journey of a thousand miles begins with a single step., Output: The journey of a thousand miles begins with a single step.\n","\n","The journey of a thousand miles begins with a single step., Target: A journey begins with a step.\n","Input: To be or not to be, that is the question., Output: Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and, by opposing, end them.\n","To die, to sleep, no more, and by a sleep to say we end the heartache and the thousand natural shocks that flesh is heir to.\n","'Tis a consummation devoutly to be wished.\n","To die, to sleep, to sleep, perchance to dream, ay there's the rub, for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause.\n","There's the respect that makes calamity of so long life.\n","For who would bear the whips and scorns of time, the oppressor's wrong, the proud man's contumely, the pangs of despised love, the law's delay, the insolence of office, and the spurns that patient merit of the unworthy takes, when he himself might his quietus make with a bare bodkin?\n","Who would fardels bear, to grunt and sweat under a weary life, but that the dread of something after death, the undiscovered country from whose bourn no traveller returns, puzzles the will and makes us rather bear those ills we have than fly to others that we know not of?\n","Thus conscience does make cowards of us all, and thus the native hue of resolution is sicklied o'er with the pale cast of thought, and enterprises of great pitch and moment with this regard their currents turn awry and lose the name of action.\n","Soft you now, the fair Ophelia, nymph, in thy orisons be all my sins remembered.\n","\n","The speaker contemplates the question of whether it is better to be alive or dead. He considers the possibility of dying to escape the troubles of life, but also wonders about the dreams that might come in death. He also considers the fear of the unknown that keeps people from choosing death. Ultimately, he concludes that fear and thought can prevent people from taking action. He then addresses Ophelia, asking her to remember him in her prayers., Target: To be or not to be.\n","Input: All that glitters is not gold., Output: The world is full of people who are not what they seem.\n","\n","Not everything that looks valuable is actually valuable.\n","The world is full of people who are not as they appear., Target: Not all that glitters is gold.\n","and the current task description:\n","Task: Summarize the text.\n","and the current system prompt:\n","Summarize the following sentences.\n","Optimize the system prompt to achieve better alignment with the target outputs.\n","Only provide the new system_prompt, nothing else\n","Only provide one system_prompt, and only provide the text of the system_prompt itself (e.g., without System Prompt: or system_prompt:)\n","\"\"\"\n","output = llm_caller.query_model(input_text, \"\")\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3pefRAthEelz","executionInfo":{"status":"ok","timestamp":1734178373788,"user_tz":-60,"elapsed":439,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"34e8d785-e0df-4fa5-eb99-6bed7574fe0d"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["system_prompt:\n","\"Summarize the text by identifying the main idea and condensing it into a shorter form.\"\n"]}]},{"cell_type":"code","source":["print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-fXNdSryIyo","executionInfo":{"status":"ok","timestamp":1734178166350,"user_tz":-60,"elapsed":7,"user":{"displayName":"Alberto Barbado","userId":"01473263592719413703"}},"outputId":"30f7073d-34ef-4c57-c651-3a63e5747403"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["System prompt:\n","Summarize the following sentences, focusing on the main ideas and themes.\n","\n","Target: The cat sat on the mat.\n","\n","System prompt:\n","Summarize the main ideas and actions in the sentences.\n","\n","Target: The cat sat on the mat, but when the dog chased it, the cat jumped off the mat and ran up a tree. The dog barked at the cat, which meowed back. The dog whined and then whimpered.\n","\n","System prompt:\n","Summarize the main idea or action in each sentence.\n","\n","Target: The quick brown fox jumps over the lazy dog, with the fox being quick and brown and the dog being lazy.\n","\n","System prompt:\n","Summarize the main idea or action in each sentence, focusing on the most important or interesting points.\n","\n","Target: The journey of a thousand miles begins with a single step.\n","\n","System prompt:\n","Summarize the main idea or action in each sentence, focusing on the most important or interesting points and using your own words.\n","\n","Target: Whether 'tis nobler in the mind to suffer the slings and arrows of outrageous fortune, or to take arms against a sea of troubles and, by opposing, end them. To die, to sleep, no more, and by a sleep to say we end the heartache and the thousand natural shocks that flesh is heir to. 'Tis a consummation devoutly to be wished. To die, to sleep, to sleep, perchance to dream, ay there's the rub, for in that sleep of death what dreams may come, when we have shuffled off this mortal coil, must give us pause. There's the respect that makes calamity of so long life. For who would bear the whips and scorns of time, the oppressor's wrong, the proud man's contumely, the pangs of despised love, the law's delay, the insolence of office, and the spurns that patient merit of the unworthy takes, when he himself might his quietus make with a bare bodkin? Who would fardels bear, to grunt and sweat under a weary life, but that the dread of something after death, the undiscovered country from whose bourn no traveller returns, puzzles the will and makes us rather bear those ills we have than fly to others that we know not of? Thus conscience does make cowards of us all, and thus the native hue of resolution is sicklied o'er with the pale cast of thought, and enterprises of great pitch and moment with this regard their currents turn awry and lose the name of action. Soft you now, the fair Ophelia, nymph, in thy orisons be all my sins remembered.\n","\n","System prompt:\n","Summarize the main idea or action in each sentence, focusing on the most important or interesting points and using your own words.\n","\n","Target: The world is full of people who are not as they appear.\n","\n","System prompt:\n","Summarize the main idea or action in each sentence, focusing on the most important or interesting points and using your own words.\n","\n","Target: Not everything that looks valuable is actually valuable. The world is full of people who are not as they appear.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HzTNJeqLyI05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"T7c_Xzh-yI3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qc7QUZSlyI5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"k-ZLgqKKyI7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"aSaR07wGyI9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AcsfBASErLDf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_cvIVCUBrLGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SnI37jjOrLIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RIZbquj9rLLA"},"execution_count":null,"outputs":[]}]}