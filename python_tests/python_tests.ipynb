{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NuK_BQmNAXc7"
      },
      "outputs": [],
      "source": [
        "import pytest\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import r2_score\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Initial configuration"
      ],
      "metadata": {
        "id": "9zb9jmvHIl8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLLinearModel:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      alpha:float,\n",
        "      l1_ratio:float,\n",
        "      random_state=0,\n",
        "      model_name=\"\"\n",
        "      ):\n",
        "\n",
        "    # Store params\n",
        "    self.alpha = alpha\n",
        "    self.l1_ratio = l1_ratio\n",
        "    self.random_state = random_state\n",
        "    self.model_name = model_name\n",
        "\n",
        "    # Initializate parameters\n",
        "    self.model_reg = None\n",
        "\n",
        "  def train_model(self, X_train:np.ndarray, y_train:np.ndarray, verbose=True):\n",
        "    self.model_reg = ElasticNet(random_state=self.random_state, alpha=self.alpha, l1_ratio=self.l1_ratio)\n",
        "    self.model_reg.fit(X_train, y_train)\n",
        "    if verbose:\n",
        "      print(self.model_reg.coef_)\n",
        "      print(self.model_reg.intercept_)\n",
        "\n",
        "  def predict_model(self, X_test:np.ndarray):\n",
        "    y_pred = self.model_reg.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "  def evaluate_model(self, X_test:np.ndarray, y_test:np.ndarray):\n",
        "    y_pred = self.predict_model(X_test)\n",
        "    return {'r2_score': r2_score(y_test, y_pred)}"
      ],
      "metadata": {
        "id": "xgh9uc_UGsGd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "n_features = 4\n",
        "X_train, y_train = make_regression(n_features=n_features, random_state=0)\n",
        "X_test, y_test = make_regression(n_features=n_features, random_state=0)"
      ],
      "metadata": {
        "id": "WHuskzjJIo6L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try class\n",
        "model_reg =  MLLinearModel(alpha=1.0, l1_ratio=0.5,model_name=\"prueba\")\n",
        "model_reg.train_model(X_train=X_train, y_train=y_train)\n",
        "model_reg.evaluate_model(X_test=X_test, y_test=y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyKqeLdsIo8a",
        "outputId": "1e2380b7-e61f-4d91-f4e5-f21531b8454f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[17.01756265 25.92033077 42.98727128 57.7739761 ]\n",
            "-0.7022844637724632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'r2_score': 0.8905284935984247}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1. Logging\n",
        "\n",
        "Source:\n",
        "* https://realpython.com/python-logging/"
      ],
      "metadata": {
        "id": "BZGesKhL4IAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logging_example.py\n",
        "\n",
        "import logging\n",
        "\n",
        "# Create a custom logger\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create handlers\n",
        "c_handler = logging.StreamHandler()\n",
        "f_handler = logging.FileHandler('file.log')\n",
        "c_handler.setLevel(logging.WARNING)\n",
        "f_handler.setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "# Create formatters and add it to handlers\n",
        "c_format = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
        "f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "#f_format = logging.Formatter('[%(asctime)s] p%(process)s {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s','%m-%d %H:%M:%S')\n",
        "\n",
        "c_handler.setFormatter(c_format)\n",
        "f_handler.setFormatter(f_format)\n",
        "\n",
        "# Add handlers to the logger\n",
        "logger.addHandler(c_handler)\n",
        "logger.addHandler(f_handler)\n",
        "\n",
        "logger.warning('This is a warning')\n",
        "logger.error('This is an error')\n",
        "logger.debug('This is a debug message')\n",
        "logger.info('This is an info message')\n",
        "logger.warning('This is a warning message')\n",
        "logger.error('This is an error message')\n",
        "logger.critical('This is a critical message')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYTuO4zB6BrA",
        "outputId": "df27f6ff-ab7b-4952-83a2-fb6f0d8f28dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "__main__ - WARNING - This is a warning\n",
            "WARNING:__main__:This is a warning\n",
            "__main__ - ERROR - This is an error\n",
            "ERROR:__main__:This is an error\n",
            "__main__ - WARNING - This is a warning message\n",
            "WARNING:__main__:This is a warning message\n",
            "__main__ - ERROR - This is an error message\n",
            "ERROR:__main__:This is an error message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "CRITICAL:__main__:This is a critical message\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger.critical('Prueba')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxrK4vJS4G_G",
        "outputId": "6be7597f-bcff-4a94-91c9-9fc5ec71d18b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "__main__ - CRITICAL - Prueba\n",
            "CRITICAL:__main__:Prueba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# logging_example.py\n",
        "\n",
        "import logging\n",
        "\n",
        "# Create a custom logger\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Create handlers\n",
        "f_handler = logging.FileHandler('file_2.log')\n",
        "f_handler.setLevel(logging.ERROR)\n",
        "f_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "# Create formatters and add it to handlers\n",
        "f_format = logging.Formatter('[%(asctime)s] p%(process)s {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s','%m-%d %H:%M:%S')\n",
        "\n",
        "f_handler.setFormatter(f_format)\n",
        "\n",
        "# Add handlers to the logger\n",
        "logger.addHandler(f_handler)\n",
        "\n",
        "logger.warning('This is a warning')\n",
        "logger.error('This is an error')\n",
        "logger.debug('This is a debug message')\n",
        "logger.info('This is an info message')\n",
        "logger.warning('This is a warning message')\n",
        "logger.error('This is an error message')\n",
        "logger.critical('This is a critical message')\n",
        "logger.critical('Prueba')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sl__ORl4HBw",
        "outputId": "9e9fefe4-9cb7-4c5e-b782-d63d2db43871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "__main__ - WARNING - This is a warning\n",
            "__main__ - WARNING - This is a warning\n",
            "__main__ - WARNING - This is a warning\n",
            "__main__ - WARNING - This is a warning\n",
            "__main__ - WARNING - This is a warning\n",
            "__main__ - WARNING - This is a warning\n",
            "__main__ - WARNING - This is a warning\n",
            "WARNING:__main__:This is a warning\n",
            "__main__ - ERROR - This is an error\n",
            "__main__ - ERROR - This is an error\n",
            "__main__ - ERROR - This is an error\n",
            "__main__ - ERROR - This is an error\n",
            "__main__ - ERROR - This is an error\n",
            "__main__ - ERROR - This is an error\n",
            "__main__ - ERROR - This is an error\n",
            "ERROR:__main__:This is an error\n",
            "__main__ - WARNING - This is a warning message\n",
            "__main__ - WARNING - This is a warning message\n",
            "__main__ - WARNING - This is a warning message\n",
            "__main__ - WARNING - This is a warning message\n",
            "__main__ - WARNING - This is a warning message\n",
            "__main__ - WARNING - This is a warning message\n",
            "__main__ - WARNING - This is a warning message\n",
            "WARNING:__main__:This is a warning message\n",
            "__main__ - ERROR - This is an error message\n",
            "__main__ - ERROR - This is an error message\n",
            "__main__ - ERROR - This is an error message\n",
            "__main__ - ERROR - This is an error message\n",
            "__main__ - ERROR - This is an error message\n",
            "__main__ - ERROR - This is an error message\n",
            "__main__ - ERROR - This is an error message\n",
            "ERROR:__main__:This is an error message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "__main__ - CRITICAL - This is a critical message\n",
            "CRITICAL:__main__:This is a critical message\n",
            "__main__ - CRITICAL - Prueba\n",
            "__main__ - CRITICAL - Prueba\n",
            "__main__ - CRITICAL - Prueba\n",
            "__main__ - CRITICAL - Prueba\n",
            "__main__ - CRITICAL - Prueba\n",
            "__main__ - CRITICAL - Prueba\n",
            "__main__ - CRITICAL - Prueba\n",
            "CRITICAL:__main__:Prueba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gAIL9e6M4mHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i4aJuVI54mJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwrxCt5M4mMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Unit Tests\n",
        "\n",
        "\"In computer programming, unit testing is a method by which individual units of source code are tested to determine if they are fit for use. A unit is the smallest testable part of an application. In procedural programming a unit may be an individual function or procedure\" [1] Thus, for instance, they test if a function or class method behaves as expected.\n",
        "\n",
        "\n",
        "References:\n",
        "[1]: https://en.wikibooks.org/wiki/Introduction_to_Software_Engineering/Testing/Unit_Tests"
      ],
      "metadata": {
        "id": "wuPMTXLvAisD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Initial example"
      ],
      "metadata": {
        "id": "BQgAY10UJmpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLModel:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      param1:float,\n",
        "      param2:float,\n",
        "      model_name:str\n",
        "      ):\n",
        "\n",
        "    # Store params\n",
        "    self.param1 = param1\n",
        "\n",
        "  def predict_model(self):\n",
        "    pass\n",
        "\n",
        "  def evaluate_model(self):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "6k1CWx6fAh3j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define several tests within a function, and evaluate them over different\n",
        "# configurations\n",
        "def test_answer(result_test):\n",
        "    assert result_test.param1 >= 0\n",
        "    assert result_test.param2 >= 0\n",
        "    assert type(result_test.param1) == float\n",
        "    assert type(result_test.param2) == float\n",
        "\n",
        "def test1():\n",
        "    return MLModel(param1=1.1, param2=1.2, model_name=\"prueba\")\n",
        "\n",
        "def test2():\n",
        "    return MLModel(param1=1.1, param2=-1, model_name=\"prueba\")\n",
        "\n",
        "# Test 1\n",
        "test_answer(result_test = test1())\n",
        "\n",
        "# Test 2\n",
        "test_answer(result_test = test2())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "EuXeni1BFN8P",
        "outputId": "9f95cd24-94f4-454d-b11c-1ec6b76ae626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d437937d67d6>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Test 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtest_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-d437937d67d6>\u001b[0m in \u001b[0;36mtest_answer\u001b[0;34m(result_test)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mresult_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam1\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mresult_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam2\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@pytest.fixture()\n",
        "def test1():\n",
        "    return MLModel(param1=1.1, param2=1.2, model_name=\"prueba\")\n",
        "\n",
        "def test_unit_create_model(test1):\n",
        "    #assert test1.model_name == \"prueba\"\n",
        "    assert test1.param1 >= 0\n",
        "    assert test1.param2 >= 0\n",
        "    assert test1.param1 == float\n",
        "    assert test1.param2 == float"
      ],
      "metadata": {
        "id": "6WcoFMXdAh6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_unit_create_model(test1)"
      ],
      "metadata": {
        "id": "MIlYEX4jAh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLModel(param1=1.1, param2=1.2, model_name=\"prueba\").model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZO6jeGq6Ah-7",
        "outputId": "999b363c-c458-4321-cf85-a12c418ea1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'prueba'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Another example\n",
        "Source:\n",
        "* https://github.com/miguelgfierro/pybase/blob/main/test/pytest_fixtures.py\n",
        "* https://docs.pytest.org/en/6.2.x/fixture.html"
      ],
      "metadata": {
        "id": "jRkjEFwOy7Nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data structures\n",
        "@pytest.fixture()\n",
        "def basic_structures():\n",
        "    data = {\n",
        "        \"int\": 5,\n",
        "        \"yes\": True,\n",
        "        \"no\": False,\n",
        "        \"float\": 0.5,\n",
        "        \"pi\": 3.141592653589793238462643383279,\n",
        "        \"string\": \"Name\",\n",
        "        \"none\": None,\n",
        "    }\n",
        "    return data\n",
        "\n",
        "@pytest.fixture()\n",
        "def complex_structures():\n",
        "    my_list = [1, 2, 3]\n",
        "    my_dict = {\"a\": 1, \"b\": 2}\n",
        "    return my_list, my_dict\n",
        "\n",
        "@pytest.fixture()\n",
        "def numeric_libs(complex_structures):\n",
        "    l, d = complex_structures\n",
        "    np_array = np.array(l)\n",
        "    df = pd.DataFrame(d, index=[0])\n",
        "    series = pd.Series(l)\n",
        "    return np_array, df, series"
      ],
      "metadata": {
        "id": "GzKDb8-IAiBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tests\n",
        "def test_basic_structures(basic_structures):\n",
        "    assert basic_structures[\"int\"] == 5\n",
        "    assert basic_structures[\"yes\"] is True\n",
        "    assert basic_structures[\"no\"] is False\n",
        "    assert basic_structures[\"float\"] == 0.5\n",
        "    assert basic_structures[\"string\"] == \"Name\"\n",
        "    assert basic_structures[\"none\"] is None\n",
        "\n",
        "\n",
        "def test_comparing_numbers(basic_structures):\n",
        "    assert basic_structures[\"pi\"] == pytest.approx(3.1415926, 0.0000001)\n",
        "    assert basic_structures[\"pi\"] != pytest.approx(3.1415926, 0.00000001)\n",
        "    assert basic_structures[\"int\"] > 3\n",
        "    assert basic_structures[\"int\"] >= 5\n",
        "    assert basic_structures[\"int\"] < 10\n",
        "    assert basic_structures[\"int\"] <= 5\n",
        "\n",
        "\n",
        "def test_lists(complex_structures):\n",
        "    l = complex_structures[0]\n",
        "    assert l == [1, 2, 3]\n",
        "    assert Counter(l) == Counter([2, 1, 3])  # list have same elements\n",
        "    assert 1 in l\n",
        "    assert 5 not in l\n",
        "    assert all(x in l for x in [2, 3])  # sublist in list\n",
        "\n",
        "\n",
        "def test_dictionaries(complex_structures):\n",
        "    d = complex_structures[1]\n",
        "    assert d == {\"a\": 1, \"b\": 2}\n",
        "    assert \"a\" in d\n",
        "    assert d.items() <= {\"a\": 1, \"b\": 2, \"c\": 3}.items()  # subdict in dict\n",
        "    with pytest.raises(KeyError):\n",
        "        value = d[\"c\"]\n",
        "\n",
        "\n",
        "def test_pandas(numeric_libs):\n",
        "    _, df, series = numeric_libs\n",
        "    df_target = pd.DataFrame({\"a\": 1, \"b\": 2}, index=[0])\n",
        "    series_target = pd.Series([1, 2, 3])\n",
        "    pd.testing.assert_frame_equal(df, df_target)\n",
        "    pd.testing.assert_series_equal(series, series_target)\n",
        "\n",
        "\n",
        "def test_numpy(numeric_libs):\n",
        "    np_array = numeric_libs[0]\n",
        "    np_target = np.array([1, 2, 3])\n",
        "    np_target2 = np.array([0.9999, 2, 3])\n",
        "    assert np.all(np_array == np_target)\n",
        "    np.testing.assert_array_equal(np_array, np_target)  # same as before\n",
        "    np.testing.assert_array_almost_equal(np_array, np_target2, decimal=4)"
      ],
      "metadata": {
        "id": "IS-BvUNrAiD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Smoke Tests\n",
        "\n",
        "\"Preliminary testing or sanity testing to reveal simple failures severe enough to, for example, reject a prospective software release. Smoke tests are a subset of test cases that cover the most important functionality of a component or system, used to aid assessment of whether main functions of the software appear to work correctly\" [1]\n",
        "\n",
        "Thus, they are used to ensure that critical components/parts of the system work, so that they can be used in production systems to quickly check that there are no obvious failures [2]\n",
        "\n",
        "\n",
        "References:\n",
        "* [1]: https://en.wikipedia.org/wiki/Smoke_testing_(software)\n",
        "* [2]: https://miguelgfierro.com/blog/2018/a-beginners-guide-to-python-testing/"
      ],
      "metadata": {
        "id": "toSytWynC2hF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from functools import wraps"
      ],
      "metadata": {
        "id": "IQnff-Qb-cb9"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_logger(logger):\n",
        "    \"\"\"\n",
        "    Decorator for unitest that output assertion Error In a logger file\n",
        "    Source: https://stackoverflow.com/questions/61346627/how-to-make-assert-output-be-logged-to-file\n",
        "\n",
        "    :param logger: a logger\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    def deco(f):\n",
        "        @wraps(f)\n",
        "        def wrapper(*args):\n",
        "            try:\n",
        "                f(*args)\n",
        "            except AssertionError as assErr:\n",
        "                # f_name = (f.__name__) f_name is the function name of the function being decorated could be usefull\n",
        "                # assErr is the message of the assertion Error\n",
        "                # you can do more formatting here\n",
        "                logger.error(assErr)  # send to the log\n",
        "\n",
        "                raise  # if you don't raise the Error the test will be a success\n",
        "\n",
        "\n",
        "        return wrapper\n",
        "    return deco"
      ],
      "metadata": {
        "id": "wt8VJ_qG-bb-"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmokeTests(unittest.TestCase):\n",
        "\n",
        "    def _has_method(self, o, name):\n",
        "      return callable(getattr(o, name, None))\n",
        "\n",
        "    def check_methods_exist(self, ModelClass):\n",
        "      logger.info(\"Ensuring model has all methods\")\n",
        "      self.assertTrue(hasattr(ModelClass, \"predict_model\") and callable(getattr(MLLinearModel, \"predict_model\")))\n",
        "      self.assertTrue(hasattr(ModelClass, \"train_model\") and callable(getattr(MLLinearModel, \"train_model\")))\n",
        "      self.assertTrue(hasattr(ModelClass, \"evaluate_model\") and callable(getattr(MLLinearModel, \"evaluate_model\")))\n",
        "\n",
        "    @test_logger(logger)\n",
        "    def check_methods_exist_v1(self, ModelClass):\n",
        "      logger.info(\"Ensuring model has all methods\")\n",
        "      list_methods = ['predict_model', 'train_model', 'evaluate_model']\n",
        "      for method_iter in list_methods:\n",
        "        test_iter = self._has_method(ModelClass, method_iter)\n",
        "        #self.assertEqual(test_iter, True, f\"Method {method_iter} does not exist\")\n",
        "        self.assertTrue(test_iter, f\"Method {method_iter} does not exist\")"
      ],
      "metadata": {
        "id": "0NzGzU_a29AB"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test OK\n",
        "SmokeTests().check_methods_exist(MLLinearModel)"
      ],
      "metadata": {
        "id": "lCrFMiwp29Ci"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test OK\n",
        "SmokeTests().check_methods_exist_v1(MLLinearModel)"
      ],
      "metadata": {
        "id": "Y1RTMn-P0Ip-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Not OK\n",
        "SmokeTests().check_methods_exist(MLModel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "3YG5oAUIx2kK",
        "outputId": "0a8a38a1-5568-4c05-8844-486fe79b51df"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "False is not true",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-c55c82571d5c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test Not OK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSmokeTests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_methods_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-6b87bf50fa97>\u001b[0m in \u001b[0;36mcheck_methods_exist\u001b[0;34m(self, ModelClass)\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ensuring model has all methods\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLLinearModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"predict_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLLinearModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"evaluate_model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLLinearModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"evaluate_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/unittest/case.py\u001b[0m in \u001b[0;36massertTrue\u001b[0;34m(self, expr, msg)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s is not true\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msafe_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: False is not true"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Not OK\n",
        "SmokeTests().check_methods_exist_v1(MLModel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "37mS59ioyW8y",
        "outputId": "e1a5b3b7-d6e7-4f66-9000-04543679aa04"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "__main__ - ERROR - False is not true : Method train_model does not exist\n",
            "ERROR:__main__:False is not true : Method train_model does not exist\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "False is not true : Method train_model does not exist",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-e61c1b0465fc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test Not OK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSmokeTests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_methods_exist_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-6b87bf50fa97>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0massErr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;31m# f_name = (f.__name__) f_name is the function name of the function being decorated could be usefull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-6b87bf50fa97>\u001b[0m in \u001b[0;36mcheck_methods_exist_v1\u001b[0;34m(self, ModelClass)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m#self.assertEqual(test_iter, True, f\"Method {method_iter} does not exist\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massertTrue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Method {method_iter} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/unittest/case.py\u001b[0m in \u001b[0;36massertTrue\u001b[0;34m(self, expr, msg)\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%s is not true\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msafe_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: False is not true : Method train_model does not exist"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Integration/Functional Tests\n",
        "\n",
        "\"Integration and functional tests are used to check the correct behavior of a system. E.g., In the case of a neural network algorithm, we would like to make sure that for a known dataset we always get a certain value of accuracy\" [1]\n",
        "\n",
        "References:\n",
        "[1]: https://miguelgfierro.com/blog/2018/a-beginners-guide-to-python-testing/"
      ],
      "metadata": {
        "id": "ef-pCo-bC4f6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest"
      ],
      "metadata": {
        "id": "intBPjcw3yJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data\n",
        "n_features = 4\n",
        "X_train, y_train = make_regression(n_features=n_features, random_state=0)\n",
        "X_test, y_test = make_regression(n_features=n_features, random_state=0)"
      ],
      "metadata": {
        "id": "fA0oxdgT29E_"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try class\n",
        "model_reg =  MLLinearModel(alpha=1.0, l1_ratio=0.5,model_name=\"prueba\")\n",
        "model_reg.train_model(X_train=X_train, y_train=y_train)\n",
        "model_reg.evaluate_model(X_test=X_test, y_test=y_test)"
      ],
      "metadata": {
        "id": "BJ5lpwha29He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pytest.mark.parametrize('alpha, l1_ratio, n_features, random_state, r2_score', [\n",
        "    (1.0, 0.5, 4, 0, 0.89),\n",
        "    #(1.0, 0.5, 4, 0, 0.89),\n",
        "])\n",
        "def test_integration_r2_score(alpha, l1_ratio, n_features, random_state, r2_score):\n",
        "  # Generate synthetic data\n",
        "  X_train, y_train = make_regression(n_features=n_features, random_state=random_state)\n",
        "  X_test, y_test = make_regression(n_features=n_features, random_state=random_state)\n",
        "\n",
        "  # Train model\n",
        "  model_reg =  MLLinearModel(alpha=alpha, l1_ratio=l1_ratio,model_name=\"prueba\")\n",
        "  model_reg.train_model(X_train=X_train, y_train=y_train, verbose=False)\n",
        "\n",
        "  # Evaluation\n",
        "  r2_score_test = np.round(model_reg.evaluate_model(X_test=X_test, y_test=y_test)['r2_score'], 2)\n",
        "  assert r2_score_test >= r2_score"
      ],
      "metadata": {
        "id": "_TJ_6fYQ2FFY"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestIntegration(unittest.TestCase):\n",
        "\n",
        "  def test_integration_r2_score(self, alpha, l1_ratio, n_features, random_state, r2_score):\n",
        "    # Generate synthetic data\n",
        "    X_train, y_train = make_regression(n_features=n_features, random_state=random_state)\n",
        "    X_test, y_test = make_regression(n_features=n_features, random_state=random_state)\n",
        "\n",
        "    # Train model\n",
        "    model_reg =  MLLinearModel(alpha=alpha, l1_ratio=l1_ratio,model_name=\"prueba\")\n",
        "    model_reg.train_model(X_train=X_train, y_train=y_train, verbose=False)\n",
        "\n",
        "    # Evaluation\n",
        "    r2_score_test = np.round(model_reg.evaluate_model(X_test=X_test, y_test=y_test)['r2_score'], 2)\n",
        "    self.assertAlmostEqual(\n",
        "        r2_score_test,\n",
        "        r2_score,\n",
        "        places=3,\n",
        "        msg=f\"Value obtained is {r2_score_test} different from {r2_score}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "bpyIg7Fu4v9a"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test OK\n",
        "test_integration_r2_score(alpha=1.0, l1_ratio=0.5, n_features=4, random_state=0, r2_score=0.89)"
      ],
      "metadata": {
        "id": "Bv-8JR3-3IJm"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test OK\n",
        "TestIntegration().test_integration_r2_score(alpha=1.0, l1_ratio=0.5, n_features=4, random_state=0, r2_score=0.89)"
      ],
      "metadata": {
        "id": "In16k6PJ5Hac"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test NOT OK\n",
        "test_integration_r2_score(alpha=1.0, l1_ratio=0.5, n_features=4, random_state=0, r2_score=0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "XPT0htC13pYe",
        "outputId": "b8264eae-184b-42d7-e9a5-76d94c716282"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-a3506689c374>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test NOT OK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_integration_r2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-1f6a6c47d9e9>\u001b[0m in \u001b[0;36mtest_integration_r2_score\u001b[0;34m(alpha, l1_ratio, n_features, random_state, r2_score)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mr2_score_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r2_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mr2_score_test\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test NOT OK\n",
        "TestIntegration().test_integration_r2_score(alpha=1.0, l1_ratio=0.5, n_features=4, random_state=0, r2_score=0.99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "gglrSfmJ5LPz",
        "outputId": "55f55645-0a4c-4d9d-e58a-1b0d5abcd91d"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "0.89 != 0.99 within 3 places (0.09999999999999998 difference) : Value obtained is 0.89 different from 0.99",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-48f1c53e07bc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test NOT OK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTestIntegration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_integration_r2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-99-a7c2db758507>\u001b[0m in \u001b[0;36mtest_integration_r2_score\u001b[0;34m(self, alpha, l1_ratio, n_features, random_state, r2_score)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mr2_score_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'r2_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     self.assertAlmostEqual(\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mr2_score_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mr2_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/unittest/case.py\u001b[0m in \u001b[0;36massertAlmostEqual\u001b[0;34m(self, first, second, places, msg, delta)\u001b[0m\n\u001b[1;32m    897\u001b[0m                 safe_repr(diff))\n\u001b[1;32m    898\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_formatMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandardMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailureException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     def assertNotAlmostEqual(self, first, second, places=None, msg=None,\n",
            "\u001b[0;31mAssertionError\u001b[0m: 0.89 != 0.99 within 3 places (0.09999999999999998 difference) : Value obtained is 0.89 different from 0.99"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Utility Test\n",
        "\n",
        "Examples of tests that show how to implement the code."
      ],
      "metadata": {
        "id": "U3Mo8Sy4C8_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLLinearModel:\n",
        "  \"\"\"\n",
        "    Class for training, prediciton and evaluation an ML regressor model (ElasticNet)\n",
        "\n",
        "    >> # Generate synthetic data\n",
        "    >> n_features = 4\n",
        "    >> X_train, y_train = make_regression(n_features=n_features, random_state=0)\n",
        "    >> X_test, y_test = make_regression(n_features=n_features, random_state=0)\n",
        "\n",
        "    >> # Try class\n",
        "    >> model_reg =  MLLinearModel(alpha=1.0, l1_ratio=0.5,model_name=\"prueba\")\n",
        "    >> model_reg.train_model(X_train=X_train, y_train=y_train)\n",
        "    >> model_reg.evaluate_model(X_test=X_test, y_test=y_test)\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      alpha:float,\n",
        "      l1_ratio:float,\n",
        "      random_state=0,\n",
        "      model_name=\"\"\n",
        "      ):\n",
        "\n",
        "    # Store params\n",
        "    self.alpha = alpha\n",
        "    self.l1_ratio = l1_ratio\n",
        "    self.random_state = random_state\n",
        "    self.model_name = model_name\n",
        "\n",
        "    # Initializate parameters\n",
        "    self.model_reg = None\n",
        "\n",
        "  def train_model(self, X_train:np.ndarray, y_train:np.ndarray, verbose=True):\n",
        "    self.model_reg = ElasticNet(random_state=self.random_state, alpha=self.alpha, l1_ratio=self.l1_ratio)\n",
        "    self.model_reg.fit(X_train, y_train)\n",
        "    if verbose:\n",
        "      print(self.model_reg.coef_)\n",
        "      print(self.model_reg.intercept_)\n",
        "\n",
        "  def predict_model(self, X_test:np.ndarray):\n",
        "    y_pred = self.model_reg.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "  def evaluate_model(self, X_test:np.ndarray, y_test:np.ndarray):\n",
        "    y_pred = self.predict_model(X_test)\n",
        "    return {'r2_score': r2_score(y_test, y_pred)}"
      ],
      "metadata": {
        "id": "fCajppiqC8hJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(MLLinearModel.__doc__)"
      ],
      "metadata": {
        "id": "plooxSm-29J1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "250823fb-2344-403e-d29a-54262f455b8d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Class for training, prediciton and evaluation an ML regressor model (ElasticNet)\n",
            "\n",
            "    >> # Generate synthetic data\n",
            "    >> n_features = 4\n",
            "    >> X_train, y_train = make_regression(n_features=n_features, random_state=0)\n",
            "    >> X_test, y_test = make_regression(n_features=n_features, random_state=0)\n",
            "\n",
            "    >> # Try class\n",
            "    >> model_reg =  MLLinearModel(alpha=1.0, l1_ratio=0.5,model_name=\"prueba\")\n",
            "    >> model_reg.train_model(X_train=X_train, y_train=y_train)\n",
            "    >> model_reg.evaluate_model(X_test=X_test, y_test=y_test)\n",
            "\n",
            "  \n"
          ]
        }
      ]
    }
  ]
}